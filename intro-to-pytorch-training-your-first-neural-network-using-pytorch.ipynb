{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0e7212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pyimagesearch.com/2021/07/12/intro-to-pytorch-training-your-first-neural-network-using-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8204f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 1.8.2+cu102\n",
      "Torchvision: 0.9.2+cu102\n",
      "GPU: 1 Quadro RTX 3000 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Torchvision:\", torchvision.__version__)\n",
    "print(\"GPU:\", torch.cuda.device_count(), torch.cuda.get_device_name(0)) if torch.cuda.is_available() else print(\"NO GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3652cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_model(inFeatures=4, hiddenDim=8, nbClasses=3):\n",
    "    # construct a shallow, sequential neural network\n",
    "    mlpModel = nn.Sequential(OrderedDict([\n",
    "        (\"hidden_layer_1\", nn.Linear(inFeatures, hiddenDim)),\n",
    "        (\"activation_1\", nn.ReLU()),\n",
    "        (\"output_layer\", nn.Linear(hiddenDim, nbClasses))\n",
    "    ]))\n",
    "\n",
    "    # return the sequential model\n",
    "    return mlpModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0296ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(inputs, targets, batchSize):\n",
    "    # loop over the dataset\n",
    "    for i in range(0, inputs.shape[0], batchSize):\n",
    "        # yield a tuple of the current batched data and labels\n",
    "        yield (inputs[i:i + batchSize], targets[i:i + batchSize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbd6b26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training using cuda...\n"
     ]
    }
   ],
   "source": [
    "# specify our batch size, number of epochs, and learning rate\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LR = 1e-2\n",
    "# determine the device we will be using for training\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"[INFO] training using {}...\".format(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4618bb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] preparing data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -2.17167054,   9.62654054,   3.41543784,   1.64007163],\n",
       "       [  1.810689  ,   7.40108036,   1.64578049,   1.6343025 ],\n",
       "       [ -7.60943821, -10.52535379,   8.25352165,   2.40327365],\n",
       "       ...,\n",
       "       [  2.51722384,   5.19314789,   6.35146645,  -0.72698056],\n",
       "       [ -4.68901597, -11.59067221,   5.73998927,   5.6966984 ],\n",
       "       [  1.24403287,  13.39246788,   4.95230713,   8.10454284]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a 3-class classification problem with 1000 data points,\n",
    "# where each data point is a 4D feature vector\n",
    "print(\"[INFO] preparing data...\")\n",
    "(X, y) = make_blobs(n_samples=1000, n_features=4, centers=3, cluster_std=2.5, random_state=95)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d83893e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 2, 0, 0, 1, 2, 0, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check label\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "576ad501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.5596,  4.0420,  5.9104,  0.4815],\n",
       "        [-5.2407, -4.8634,  6.8165,  4.9165],\n",
       "        [-9.6785, -7.1552,  6.8649,  6.1058],\n",
       "        ...,\n",
       "        [ 2.6384,  6.6450,  3.9682, -0.7591],\n",
       "        [-5.8820, -6.9710,  6.8110,  6.8298],\n",
       "        [ 3.9124,  7.9426, -0.1382, 12.2435]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create training and testing splits, and convert them to PyTorch tensors\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, y, test_size=0.15, random_state=95)\n",
    "trainX = torch.from_numpy(trainX).float()\n",
    "testX = torch.from_numpy(testX).float()\n",
    "trainY = torch.from_numpy(trainY).float()\n",
    "testY = torch.from_numpy(testY).float()\n",
    "\n",
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "077ef171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([850, 4])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bbb9ca57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (hidden_layer_1): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (activation_1): ReLU()\n",
       "  (output_layer): Linear(in_features=8, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize our model and display its architecture\n",
    "mlp = get_training_model().to(DEVICE)\n",
    "mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5b61eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize optimizer and loss function\n",
    "opt = SGD(mlp.parameters(), lr=LR)\n",
    "lossFunc = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "04507864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch: 1...\n",
      "epoch: 1 train loss: 1.652 train accuracy: 0.421\n",
      "[INFO] epoch: 2...\n",
      "epoch: 2 train loss: 0.515 train accuracy: 0.747\n",
      "[INFO] epoch: 3...\n",
      "epoch: 3 train loss: 0.332 train accuracy: 0.879\n",
      "[INFO] epoch: 4...\n",
      "epoch: 4 train loss: 0.241 train accuracy: 0.939\n",
      "[INFO] epoch: 5...\n",
      "epoch: 5 train loss: 0.189 train accuracy: 0.965\n",
      "[INFO] epoch: 6...\n",
      "epoch: 6 train loss: 0.156 train accuracy: 0.973\n",
      "[INFO] epoch: 7...\n",
      "epoch: 7 train loss: 0.135 train accuracy: 0.974\n",
      "[INFO] epoch: 8...\n",
      "epoch: 8 train loss: 0.120 train accuracy: 0.976\n",
      "[INFO] epoch: 9...\n",
      "epoch: 9 train loss: 0.109 train accuracy: 0.976\n",
      "[INFO] epoch: 10...\n",
      "epoch: 10 train loss: 0.100 train accuracy: 0.978\n",
      "[INFO] epoch: 11...\n",
      "epoch: 11 train loss: 0.093 train accuracy: 0.981\n",
      "[INFO] epoch: 12...\n",
      "epoch: 12 train loss: 0.088 train accuracy: 0.981\n",
      "[INFO] epoch: 13...\n",
      "epoch: 13 train loss: 0.083 train accuracy: 0.981\n",
      "[INFO] epoch: 14...\n",
      "epoch: 14 train loss: 0.080 train accuracy: 0.981\n",
      "[INFO] epoch: 15...\n",
      "epoch: 15 train loss: 0.076 train accuracy: 0.981\n",
      "[INFO] epoch: 16...\n",
      "epoch: 16 train loss: 0.073 train accuracy: 0.981\n",
      "[INFO] epoch: 17...\n",
      "epoch: 17 train loss: 0.071 train accuracy: 0.982\n",
      "[INFO] epoch: 18...\n",
      "epoch: 18 train loss: 0.069 train accuracy: 0.982\n",
      "[INFO] epoch: 19...\n",
      "epoch: 19 train loss: 0.067 train accuracy: 0.982\n",
      "[INFO] epoch: 20...\n",
      "epoch: 20 train loss: 0.065 train accuracy: 0.982\n",
      "[INFO] epoch: 21...\n",
      "epoch: 21 train loss: 0.064 train accuracy: 0.984\n",
      "[INFO] epoch: 22...\n",
      "epoch: 22 train loss: 0.062 train accuracy: 0.984\n",
      "[INFO] epoch: 23...\n",
      "epoch: 23 train loss: 0.061 train accuracy: 0.984\n",
      "[INFO] epoch: 24...\n",
      "epoch: 24 train loss: 0.060 train accuracy: 0.984\n",
      "[INFO] epoch: 25...\n",
      "epoch: 25 train loss: 0.059 train accuracy: 0.984\n",
      "[INFO] epoch: 26...\n",
      "epoch: 26 train loss: 0.058 train accuracy: 0.984\n",
      "[INFO] epoch: 27...\n",
      "epoch: 27 train loss: 0.057 train accuracy: 0.984\n",
      "[INFO] epoch: 28...\n",
      "epoch: 28 train loss: 0.056 train accuracy: 0.984\n",
      "[INFO] epoch: 29...\n",
      "epoch: 29 train loss: 0.055 train accuracy: 0.984\n",
      "[INFO] epoch: 30...\n",
      "epoch: 30 train loss: 0.054 train accuracy: 0.984\n",
      "[INFO] epoch: 31...\n",
      "epoch: 31 train loss: 0.054 train accuracy: 0.984\n",
      "[INFO] epoch: 32...\n",
      "epoch: 32 train loss: 0.053 train accuracy: 0.985\n",
      "[INFO] epoch: 33...\n",
      "epoch: 33 train loss: 0.052 train accuracy: 0.985\n",
      "[INFO] epoch: 34...\n",
      "epoch: 34 train loss: 0.052 train accuracy: 0.985\n",
      "[INFO] epoch: 35...\n",
      "epoch: 35 train loss: 0.051 train accuracy: 0.985\n",
      "[INFO] epoch: 36...\n",
      "epoch: 36 train loss: 0.051 train accuracy: 0.985\n",
      "[INFO] epoch: 37...\n",
      "epoch: 37 train loss: 0.050 train accuracy: 0.985\n",
      "[INFO] epoch: 38...\n",
      "epoch: 38 train loss: 0.050 train accuracy: 0.985\n",
      "[INFO] epoch: 39...\n",
      "epoch: 39 train loss: 0.049 train accuracy: 0.985\n",
      "[INFO] epoch: 40...\n",
      "epoch: 40 train loss: 0.049 train accuracy: 0.986\n",
      "[INFO] epoch: 41...\n",
      "epoch: 41 train loss: 0.048 train accuracy: 0.986\n",
      "[INFO] epoch: 42...\n",
      "epoch: 42 train loss: 0.048 train accuracy: 0.986\n",
      "[INFO] epoch: 43...\n",
      "epoch: 43 train loss: 0.048 train accuracy: 0.986\n",
      "[INFO] epoch: 44...\n",
      "epoch: 44 train loss: 0.047 train accuracy: 0.986\n",
      "[INFO] epoch: 45...\n",
      "epoch: 45 train loss: 0.047 train accuracy: 0.986\n",
      "[INFO] epoch: 46...\n",
      "epoch: 46 train loss: 0.046 train accuracy: 0.986\n",
      "[INFO] epoch: 47...\n",
      "epoch: 47 train loss: 0.046 train accuracy: 0.986\n",
      "[INFO] epoch: 48...\n",
      "epoch: 48 train loss: 0.046 train accuracy: 0.986\n",
      "[INFO] epoch: 49...\n",
      "epoch: 49 train loss: 0.046 train accuracy: 0.986\n",
      "[INFO] epoch: 50...\n",
      "epoch: 50 train loss: 0.045 train accuracy: 0.987\n",
      "CPU times: total: 1.22 s\n",
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a template to summarize current training progress\n",
    "trainTemplate = \"epoch: {} test loss: {:.3f} test accuracy: {:.3f}\"\n",
    "# loop through the epochs\n",
    "for epoch in range(0, EPOCHS+40):\n",
    "    # initialize tracker variables and set our model to trainable\n",
    "    print(\"[INFO] epoch: {}...\".format(epoch + 1))\n",
    "    trainLoss = 0\n",
    "    trainAcc = 0\n",
    "    samples = 0\n",
    "    mlp.train()\n",
    "    # loop over the current batch of data\n",
    "    for (batchX, batchY) in next_batch(trainX, trainY, BATCH_SIZE):\n",
    "        # flash data to the current device, run it through our model, and calculate loss\n",
    "        (batchX, batchY) = (batchX.to(DEVICE), batchY.to(DEVICE))\n",
    "        predictions = mlp(batchX)\n",
    "        loss = lossFunc(predictions, batchY.long())\n",
    "        # zero the gradients accumulated from the previous steps,\n",
    "        # perform backpropagation, and update model parameters\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        # update training loss, accuracy, and the number of samples visited\n",
    "        trainLoss += loss.item() * batchY.size(0)\n",
    "        trainAcc += (predictions.max(1)[1] == batchY).sum().item()\n",
    "        samples += batchY.size(0)\n",
    "    \n",
    "    # display model progress on the current training batch\n",
    "    trainTemplate = \"epoch: {} train loss: {:.3f} train accuracy: {:.3f}\"\n",
    "    print(trainTemplate.format(epoch + 1, (trainLoss / samples), (trainAcc / samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5148c3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50 test loss: 0.050 test accuracy: 0.987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize tracker variables for testing, then set our model to evaluation mode\n",
    "testLoss = 0\n",
    "testAcc = 0\n",
    "samples = 0\n",
    "mlp.eval()\n",
    "# initialize a no-gradient context\n",
    "with torch.no_grad():\n",
    "    # loop over the current batch of test data\n",
    "    for (batchX, batchY) in next_batch(testX, testY, BATCH_SIZE):\n",
    "        # flash the data to the current device\n",
    "        (batchX, batchY) = (batchX.to(DEVICE), batchY.to(DEVICE))\n",
    "        # run data through our model and calculate loss\n",
    "        predictions = mlp(batchX)\n",
    "        loss = lossFunc(predictions, batchY.long())\n",
    "        # update test loss, accuracy, and the number of samples visited\n",
    "        testLoss += loss.item() * batchY.size(0)\n",
    "        testAcc += (predictions.max(1)[1] == batchY).sum().item()\n",
    "        samples += batchY.size(0)\n",
    "    \n",
    "    # display model progress on the current test batch\n",
    "    testTemplate = \"epoch: {} test loss: {:.3f} test accuracy: {:.3f}\"\n",
    "    print(testTemplate.format(epoch + 1, (testLoss / samples), (testAcc / samples)))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe3824b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2fae68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
